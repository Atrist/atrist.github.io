{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{547:function(t,s,a){\"use strict\";a.r(s);var n=a(5),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":t.$parent.slotKey}},[a(\"h2\",{attrs:{id:\"参考资料\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#参考资料\"}},[t._v(\"#\")]),t._v(\" 参考资料\")]),t._v(\" \"),a(\"ol\",[a(\"li\",[a(\"a\",{attrs:{href:\"https://codelabs.developers.google.com/codelabs/tfjs-training-regression/index.html?hl=zh_cn#0\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"TensorFlow 官方的二维数据进行预测的教程\"),a(\"OutboundLink\")],1)])]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_1-简介\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-简介\"}},[t._v(\"#\")]),t._v(\" 1. 简介\")]),t._v(\" \"),a(\"p\",[t._v(\"在这一次的实验中,你会训练一个模型根据一个描述汽车的二维数据中进行预测\")]),t._v(\" \"),a(\"p\",[t._v(\"该练习将演示训练许多不同模型的通用步骤，但将使用一个小的数据集和一个简单的（浅）模型。其主要目的是帮助您熟悉使用 TensorFlow.js 进行培训的模型的基本术语，概念和语法，并为进一步的探索和学习提供垫脚石。\")]),t._v(\" \"),a(\"p\",[t._v(\"因为我们正在训练一个模型来预测连续数，所以此任务有时称为\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#regression_model\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"回归\"),a(\"OutboundLink\")],1),t._v(\"任务。我们将通过显示许多输入示例以及正确的输出来训练模型。这被称为\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/problem-framing/cases\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"监督学习\"),a(\"OutboundLink\")],1),t._v(\"。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"你会建立什么\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#你会建立什么\"}},[t._v(\"#\")]),t._v(\" 你会建立什么\")]),t._v(\" \"),a(\"p\",[t._v(\"您将创建一个使用 TensorFlow.js 在浏览器中训练模型的网页。给定汽车的“马力”，模型将学习预测“每加仑英里”（MPG）。\")]),t._v(\" \"),a(\"p\",[t._v(\"为此，您将：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"加载数据并准备进行训练。\")]),t._v(\" \"),a(\"li\",[t._v(\"定义模型的架构。\")]),t._v(\" \"),a(\"li\",[t._v(\"训练模型并在训练时监视其性能\")]),t._v(\" \"),a(\"li\",[t._v(\"通过做出一些预测来评估训练后的模型。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"你会学到什么\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#你会学到什么\"}},[t._v(\"#\")]),t._v(\" 你会学到什么\")]),t._v(\" \"),a(\"ol\",[a(\"li\",[t._v(\"机器学习数据准备的最佳实践，包括混洗和规范化。\")]),t._v(\" \"),a(\"li\",[t._v(\"TensorFlow.js 语法，用于使用\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#Layers\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"tf.layers API\"),a(\"OutboundLink\")],1),t._v(\"创建模型。\")]),t._v(\" \"),a(\"li\",[t._v(\"如何使用\"),a(\"a\",{attrs:{href:\"https://github.com/tensorflow/tfjs-vis\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"tfjs-vis 库\"),a(\"OutboundLink\")],1),t._v(\"监视浏览器内训练。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"你需要准备什么\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#你需要准备什么\"}},[t._v(\"#\")]),t._v(\" 你需要准备什么\")]),t._v(\" \"),a(\"ol\",[a(\"li\",[t._v(\"最新的版本 chrome 浏览器或者其他现代浏览器\")]),t._v(\" \"),a(\"li\",[t._v(\"一个文本编辑器,推荐使用 vscode\")]),t._v(\" \"),a(\"li\",[t._v(\"了解部分 html,css,js 以及 chrome\")]),t._v(\" \"),a(\"li\",[t._v(\"对神经网络的高级概念理解。如果需要介绍或复习，请考虑观看\"),a(\"a\",{attrs:{href:\"https://www.youtube.com/watch?v=aircAruvnKk\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"3blue1brown 的视频\"),a(\"OutboundLink\")],1),t._v(\"或 A\"),a(\"a\",{attrs:{href:\"https://www.youtube.com/watch?v=SV-cgdobtTA\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"shi Krishnan 的 Java 深度学习视频\"),a(\"OutboundLink\")],1),t._v(\"。\")])]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_2-起步\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-起步\"}},[t._v(\"#\")]),t._v(\" 2. 起步\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_1-创建一个-html-文件\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-创建一个-html-文件\"}},[t._v(\"#\")]),t._v(\" 1. 创建一个 html 文件\")]),t._v(\" \"),a(\"p\",[t._v(\"将下面代码 复制到一个\"),a(\"code\",[t._v(\"index.html\")]),t._v(\"文件中\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-html line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-html\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token doctype\"}},[t._v(\"<!DOCTYPE html>\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"html\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"head\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"title\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"TensorFlow.js Tutorial\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"title\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import TensorFlow.js --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import tfjs-vis --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.0.2/dist/tfjs-vis.umd.min.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import the main script file --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"script.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"head\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"body\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"body\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"html\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\")])]),a(\"h3\",{attrs:{id:\"_2-创建一个-js-文件\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-创建一个-js-文件\"}},[t._v(\"#\")]),t._v(\" 2. 创建一个 js 文件\")]),t._v(\" \"),a(\"p\",[t._v(\"在同级目录下,创建一个 script.js 文件, 文件内容如下:\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[t._v(\"console\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"log\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Hello TensorFlow\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])]),a(\"h3\",{attrs:{id:\"_3-测试\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-测试\"}},[t._v(\"#\")]),t._v(\" 3. 测试\")]),t._v(\" \"),a(\"p\",[t._v(\"使用浏览器打开你的\"),a(\"code\",[t._v(\"index.html\")]),t._v(\"文件\")]),t._v(\" \"),a(\"p\",[t._v(\"如果一切都正常工作的话,这里有两个全局变量可以在 开发者工具中查询到\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[a(\"code\",[t._v(\"tf\")]),t._v(\"是对 TensorFlow.js 库的引用\")]),t._v(\" \"),a(\"li\",[a(\"code\",[t._v(\"tfvis\")]),t._v(\"是对 tfjs-vis 库的引用\")])]),t._v(\" \"),a(\"p\",[t._v(\"同时会有一条消息 \"),a(\"code\",[t._v(\"Hello TensorFlow\")]),t._v(\"出现在开发者工具的\"),a(\"code\",[t._v(\"console\")]),t._v(\"中\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_3-加载-格式化和可视化输入数据\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-加载-格式化和可视化输入数据\"}},[t._v(\"#\")]),t._v(\" 3. 加载,格式化和可视化输入数据\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_1-加载\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-加载\"}},[t._v(\"#\")]),t._v(\" 1. 加载\")]),t._v(\" \"),a(\"p\",[t._v(\"我们将从为您托管的 JSON 文件中加载“汽车”数据集。它包含有关每个给定汽车的许多不同功能。对于本教程，我们只想提取有关马力和每加仑英里数的数据。\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到 \"),a(\"code\",[t._v(\"script.js\")])]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n * Get the car data reduced to just the variables we are interested\\n * and cleaned of missing data.\\n */\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" carsDataReq \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fetch\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"https://storage.googleapis.com/tfjs-tutorials/carsData.json\"')]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" carsData \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" carsDataReq\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"json\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" cleaned \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" carsData\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"car\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Miles_per_Gallon\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"filter\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"car\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"&&\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" cleaned\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\")])]),a(\"p\",[t._v(\"这还将删除所有未定义每加仑英里数或马力的条目。我们还将这些数据绘制在散点图中以查看其外观。\")]),t._v(\" \"),a(\"p\",[t._v(\"将以下代码添加到 script.js 文件的底部\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"run\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Load and plot the original input data that we are going to train on.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" data \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" values \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"render\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"scatterplot\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower v MPG\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" values \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      xLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      yLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"MPG\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"300\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// More code will be added below\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\ndocument\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addEventListener\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"DOMContentLoaded\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" run\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\")])]),a(\"p\",[t._v(\"当年重新刷新你页面的时候,你会看见一个如下的界面:\\n\"),a(\"img\",{attrs:{src:\"https://codelabs.developers.google.com/codelabs/tfjs-training-regression/img/cf44e823106c758e.png\",alt:\"\"}})]),t._v(\" \"),a(\"p\",[t._v(\"该面板称为遮阳板，由\"),a(\"a\",{attrs:{href:\"https://github.com/tensorflow/tfjs-vis\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"tfjs-vis\"),a(\"OutboundLink\")],1),t._v(\"提供。它提供了一个方便的位置来显示可视化效果。\")]),t._v(\" \"),a(\"p\",[t._v(\"通常，在处理数据时，最好找到一种方法来查看数据并在必要时进行清理。在这种情况下，我们必须从 carsData 中删除某些没有所有必填字段的条目。可视化数据可以使我们了解模型是否可以学习数据的任何结构。\")]),t._v(\" \"),a(\"p\",[t._v(\"从上图可以看出，马力和 MPG 之间存在负相关关系，即，随着马力的增加，汽车每加仑行驶的里程通常会减少。\")]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"请记住：如果数据中没有任何结构（模式）（即数据是随机的），则该模型实际上将无法学习任何内容。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_2-概念化我们的任务\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-概念化我们的任务\"}},[t._v(\"#\")]),t._v(\" 2. 概念化我们的任务\")]),t._v(\" \"),a(\"p\",[t._v(\"现在，我们的输入数据将如下所示。\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"...\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mpg\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"15\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"165\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mpg\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"18\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"150\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mpg\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"16\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"150\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"...\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\")])]),a(\"p\",[t._v(\"我们的目标是训练一个模型，该模型将采用\"),a(\"strong\",[t._v(\"一个马力数\")]),t._v(\"，并学会预测\"),a(\"strong\",[t._v(\"一个数字\")]),t._v(\"，即每加仑英里数。记住一对一的映射，因为这对于下一节很重要。\")]),t._v(\" \"),a(\"p\",[t._v(\"我们将把这些示例（马力和 MPG）提供给神经网络，该神经网络将从这些示例中学习公式（或函数）来预测给定马力的 MPG。\")]),t._v(\" \"),a(\"p\",[t._v(\"我们从示例中得到正确答案的这种学习称为\"),a(\"a\",{attrs:{href:\"https://en.wikipedia.org/wiki/Supervised_learning\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"监督学习\"),a(\"OutboundLink\")],1),t._v(\"。\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_4-定义模型架构\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_4-定义模型架构\"}},[t._v(\"#\")]),t._v(\" 4. 定义模型架构\")]),t._v(\" \"),a(\"p\",[t._v(\"在本节中，我们将编写代码来描述模型架构。模型体系结构只是一种说法：“模型在执行时将运行哪些功能”，或者“模型将使用哪种算法来计算其答案”。\")]),t._v(\" \"),a(\"p\",[t._v(\"ML 模型是一种接受输入并产生输出的算法。使用神经网络时，该算法是一组神经元层，其中“权重”（数字）控制着它们的输出。训练过程将学习这些权重的理想值。\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到 \"),a(\"code\",[t._v(\"script.js\")]),t._v(\"文件中,用于定义模型架构:\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"createModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 创建一个顺序模型\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" model \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sequential\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 添加单个输入层\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputShape\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" useBias\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 添加输出层\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" useBias\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\")])]),a(\"p\",[t._v(\"这是我们可以在 tensorflow.js 中定义的最简单的模型之一，让我们细分每一行。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_1-实例化模型\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-实例化模型\"}},[t._v(\"#\")]),t._v(\" 1. 实例化模型\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" model \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sequential\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])]),a(\"p\",[t._v(\"这将实例化一个\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#class:Model\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"tf.Model\"),a(\"OutboundLink\")],1),t._v(\"对象。该模型是\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#sequential\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"顺序的\"),a(\"OutboundLink\")],1),t._v(\"，因为其输入直接向下流至其输出。其他类型的模型可以具有分支，甚至可以具有多个输入和输出，但是在许多情况下，您的模型将是顺序的。顺序模型还具有\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#class:Sequential\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"易于使用的 API\"),a(\"OutboundLink\")],1),t._v(\"。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_2-添加图层\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-添加图层\"}},[t._v(\"#\")]),t._v(\" 2. 添加图层\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputShape\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" useBias\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])]),a(\"p\",[t._v(\"这为我们的网络添加了一个输入层，该输入层通过一个隐藏的单元自动连接到“密集”层。\"),a(\"code\",[t._v(\"密集层\")]),t._v(\"是一种层的类型，它将其输入乘以一个矩阵（称为权重），然后将一个数字（称为偏差）加到结果上。由于这是网络的第一层，因此我们需要定义我们的\"),a(\"code\",[t._v(\"inputShape\")]),t._v(\"。这个\"),a(\"code\",[t._v(\"inputShape\")]),t._v(\"为\"),a(\"code\",[t._v(\"[1]\")]),t._v(\"，因为我们有 1 个数字作为输入（给定汽车的马力）。\")]),t._v(\" \"),a(\"p\",[a(\"code\",[t._v(\"units\")]),t._v(\"设置权重矩阵在图层中的大小。通过在此处将其设置为 1，我们说数据的每个输入要素将具有 1 权重。\")]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"注意：密集层默认情况下带有一个偏差项，因此我们不需要将\"),a(\"code\",[t._v(\"useBias\")]),t._v(\"设置为\"),a(\"code\",[t._v(\"true\")]),t._v(\"，我们将省略对\"),a(\"code\",[t._v(\"tf.layers.dense\")]),t._v(\"的进一步调用\")])]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])]),a(\"p\",[t._v(\"上面的代码创建了我们的输出层。我们将\"),a(\"code\",[t._v(\"units\")]),t._v(\"设置为 1，因为我们要输出 1 个数字。\")]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"注意：在此示例中，由于隐藏层具有 1 个单位，因此我们实际上不需要在上面添加最终输出层（即我们可以将隐藏层用作输出层）。但是，定义一个单独的输出层使我们可以修改隐藏层中的单元数，同时保持输入和输出的一对一映射。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_3-创建一个实例\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-创建一个实例\"}},[t._v(\"#\")]),t._v(\" 3. 创建一个实例\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到 \"),a(\"code\",[t._v(\"script.js\")]),t._v(\"的 \"),a(\"code\",[t._v(\"run\")]),t._v(\"函数中:\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Create the model\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" model \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"createModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\ntfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"show\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"modelSummary\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Model Summary\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\")])]),a(\"p\",[t._v(\"这将创建模型的实例，并显示网页上各层的摘要。\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_5-准备训练数据\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_5-准备训练数据\"}},[t._v(\"#\")]),t._v(\" 5.准备训练数据\")]),t._v(\" \"),a(\"p\",[t._v(\"为了获得使训练机器学习模型切实可行的 TensorFlow.js 的性能优势，我们需要将数据转换为\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#tensor\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"张量\"),a(\"OutboundLink\")],1),t._v(\"。我们还将对数据进行一些最佳实践的转换，即\"),a(\"strong\",[t._v(\"改组\")]),t._v(\"和\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#normalization\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"标准化\"),a(\"OutboundLink\")],1),t._v(\"。\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到 \"),a(\"code\",[t._v(\"script.js\")])]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n * Convert the input data to tensors that we can use for machine\\n * learning. We will also do the important best practices of _shuffling_\\n * the data and _normalizing_ the data\\n * MPG on the y-axis.\\n */\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"convertToTensor\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"data\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Wrapping these calculations in a tidy will dispose any\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// intermediate tensors.\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tidy\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//步骤1.随机整理数据\")]),t._v(\"\\n    tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"shuffle\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//步骤2。将数据转换为Tensor\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//第3步。使用最小-最大缩放将数据标准化为0-1的范围\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedInputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedLabels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedInputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labels\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedLabels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// //返回最小/最大范围，以便以后使用。\")]),t._v(\"\\n      inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"20\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"21\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"22\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"23\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"24\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"25\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"26\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"27\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"28\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"29\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"30\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"31\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"32\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"33\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"34\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"35\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"36\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"37\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"38\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"39\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"40\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"41\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"42\")]),a(\"br\")])]),a(\"p\",[t._v(\"让我们分解一下这里发生的事情。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_1-随机整理数据\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-随机整理数据\"}},[t._v(\"#\")]),t._v(\" 1. 随机整理数据\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Step 1. Shuffle the data\")]),t._v(\"\\ntf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"shuffle\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\")])]),a(\"p\",[t._v(\"在这里，我们随机化示例的顺序，然后将它们提供给训练算法。改组很重要，因为通常在训练过程中，数据集会被分成较小的子集，称为模型的子集。改组可帮助每个批次从整个数据分布中获取各种数据。通过这样做，我们可以帮助模型：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"不学习纯粹依赖于数据输入顺序的东西\")]),t._v(\" \"),a(\"li\",[t._v(\"对子组的结构不敏感（例如，如果在训练的前半段只看到高马力的汽车，它可能会发现一种不适用于整个数据集的关系）\")])]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"最佳实践 1：在将数据交给 TensorFlow.js 中的训练算法之前，您应该始终对数据进行清洗\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_2-转换为张量\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-转换为张量\"}},[t._v(\"#\")]),t._v(\" 2. 转换为张量\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Step 2. Convert data to Tensor\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\")])]),a(\"p\",[t._v(\"在这里，我们制作了两个数组，一个数组用于输入示例（马力条目），另一个数组用于真实的输出值（在机器学习中称为标签）。\")]),t._v(\" \"),a(\"p\",[t._v(\"然后，我们将每个数组数据转换为 2d 张量。张量的形状为\"),a(\"code\",[t._v(\"[num_examples，num_features_per_example]\")]),t._v(\"。这里我们有\"),a(\"code\",[t._v(\"input.length\")]),t._v(\"个示例，每个示例都有 1 个输入功能（马力）\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_3-规范化数据\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-规范化数据\"}},[t._v(\"#\")]),t._v(\" 3. 规范化数据\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//Step 3. Normalize the data to the range 0 - 1 using min-max scaling\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedInputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedLabels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\")])]),a(\"p\",[t._v(\"接下来，我们为机器学习培训提供另一种最佳实践。我们\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/data-prep/transform/normalization\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"标准化\"),a(\"OutboundLink\")],1),t._v(\"数据。在这里，我们使用\"),a(\"a\",{attrs:{href:\"https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"最小-最大缩放\"),a(\"OutboundLink\")],1),t._v(\"将数据归一化为数值范围 0-1。规范化很重要，因为您将使用 tensorflow.js 构建的许多机器学习模型的内部设计为可以处理不太大的数字。标准化数据的通用范围包括\"),a(\"code\",[t._v(\"0到1\")]),t._v(\"或\"),a(\"code\",[t._v(\"-1到1\")]),t._v(\"。如果您养成将数据标准化到某个合理范围的习惯，则可以在训练模型方面获得更大的成功。\")]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"最佳实践 2：您应该始终在训练之前考虑对数据进行标准化。某些数据集无需进行归一化即可学习，但是对数据进行归一化通常会消除一类阻碍有效学习的问题。\"),a(\"br\"),t._v(\"\\n您可以在将数据转换为张量之前对其进行标准化。我们之所以这样做，是因为我们可以利用 TensorFlow.js 中的矢量化优势进行最小-最大缩放操作，而无需编写任何显式的 for 循环。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_4-返回数据和归一化界限\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_4-返回数据和归一化界限\"}},[t._v(\"#\")]),t._v(\" 4. 返回数据和归一化界限\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedInputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  labels\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedLabels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Return the min/max bounds so we can use them later.\")]),t._v(\"\\n  inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\")])]),a(\"p\",[t._v(\"我们希望保留在训练期间用于标准化的值，以便我们可以对输出进行非标准化以使其恢复到原始比例，并允许我们以相同的方式标准化未来的输入数据。\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_6-训练模型\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_6-训练模型\"}},[t._v(\"#\")]),t._v(\" 6. 训练模型\")]),t._v(\" \"),a(\"p\",[t._v(\"创建模型实例并将数据表示为张量后，我们就可以开始训练过程了。\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到\"),a(\"code\",[t._v(\"script.js\")]),t._v(\"文件中\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"trainModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Prepare the model for training.\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"compile\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    optimizer\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"adam\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    loss\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"losses\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"meanSquaredError\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    metrics\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" batchSize \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"32\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" epochs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"50\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fit\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    batchSize\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    epochs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    shuffle\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"show\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fitCallbacks\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Training Performance\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"loss\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"200\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"onEpochEnd\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"20\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"21\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"22\")]),a(\"br\")])]),a(\"p\",[t._v(\"让我们分解一下。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_1-准备训练\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-准备训练\"}},[t._v(\"#\")]),t._v(\" 1. 准备训练\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Prepare the model for training.\")]),t._v(\"\\nmodel\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"compile\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  optimizer\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"adam\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  loss\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"losses\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"meanSquaredError\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  metrics\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\")])]),a(\"p\",[t._v(\"在训练模型之前，我们必须“编译”模型。为此，我们必须指定一些非常重要的内容：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#optimizer\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"优化器\"),a(\"OutboundLink\")],1),t._v(\"：如示例所示，这是将用于控制模型更新的算法。TensorFlow.js 中有许多可用的优化器。在这里，我们选择了 adam 优化器，因为它在实践中非常有效并且不需要配置。\")]),t._v(\" \"),a(\"li\",[a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#loss\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"损失\"),a(\"OutboundLink\")],1),t._v(\"：该函数将告诉模型学习所显示的每个批次（数据子集）时的性能如何。在这里，我们使用\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#MSE\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"meanSquaredError\"),a(\"OutboundLink\")],1),t._v(\"将模型所做的预测与真实值进行比较。\")])]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" batchSize \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"32\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" epochs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"50\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\")])]),a(\"p\",[t._v(\"接下来，我们选择一个 batchSize 和多个时期：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#batch_size\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"batchSize\"),a(\"OutboundLink\")],1),t._v(\":是指模型在每次训练迭代中将看到的数据子集的大小。常见的批量大小通常在 32-512 之间。对于所有问题，实际上并没有理想的批量大小，并且描述各种批量大小的数学动机超出了本教程的范围。\")]),t._v(\" \"),a(\"li\",[a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#epoch\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"epochs\"),a(\"OutboundLink\")],1),t._v(\":取决于模型将查看您提供的整个数据集的次数。在这里，我们将对数据集进行 50 次迭代。\")])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_2-开始训练循环\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-开始训练循环\"}},[t._v(\"#\")]),t._v(\" 2. 开始训练循环\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fit\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  batchSize\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  epochs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"show\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fitCallbacks\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Training Performance\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"loss\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"200\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"onEpochEnd\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\")])]),a(\"p\",[a(\"code\",[t._v(\"model.fit\")]),t._v(\"是我们用来启动训练循环的函数。它是一个异步函数，因此我们返回它给我们的\"),a(\"code\",[t._v(\"promise\")]),t._v(\"，以便调用者可以确定训练何时完成。\")]),t._v(\" \"),a(\"p\",[t._v(\"为了监视训练进度，我们将一些回调传递给\"),a(\"code\",[t._v(\"model.fit\")]),t._v(\"。我们使用\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api_vis/latest/#show.fitCallbacks\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"tfvis.show.fitCallbacks\"),a(\"OutboundLink\")],1),t._v(\"生成函数，以绘制我们先前指定的“损失”和“ mse”指标的图表。\")]),t._v(\" \"),a(\"h3\",{attrs:{id:\"_3-放在一起\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-放在一起\"}},[t._v(\"#\")]),t._v(\" 3. 放在一起\")]),t._v(\" \"),a(\"p\",[t._v(\"现在，我们必须从\"),a(\"code\",[t._v(\"run\")]),t._v(\"函数中调用已定义的函数。\")]),t._v(\" \"),a(\"p\",[t._v(\"添加如下代码到\"),a(\"code\",[t._v(\"script.js\")]),t._v(\"中\"),a(\"code\",[t._v(\"run\")]),t._v(\"函数当中:\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Convert the data to a form we can use for training.\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" tensorData \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"convertToTensor\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tensorData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Train the model\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"trainModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\nconsole\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"log\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Done Training\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\")])]),a(\"p\",[t._v(\"刷新页面时，几秒钟后，您将看到以下图形更新。\")]),t._v(\" \"),a(\"p\",[a(\"img\",{attrs:{src:\"https://codelabs.developers.google.com/codelabs/tfjs-training-regression/img/c6d3214d6e8c3752.png\",alt:\"\"}})]),t._v(\" \"),a(\"p\",[t._v(\"这些是由我们之前创建的回调创建的。它们在每个时期结束时显示整个数据集的平均损耗和 mse。\")]),t._v(\" \"),a(\"p\",[t._v(\"训练模型时，我们希望看到损失减少。在这种情况下，由于我们的度量标准是一种误差度量，因此我们也希望看到它也有所下降。\")]),t._v(\" \"),a(\"blockquote\",[a(\"p\",[t._v(\"如果您想在训练时了解幕后情况。阅读我们的指南或观看\"),a(\"a\",{attrs:{href:\"https://www.youtube.com/watch?v=IHZwWFHWa-w\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"3blue1brown 的视频\"),a(\"OutboundLink\")],1),t._v(\"。\")])]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_7-作出预测\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_7-作出预测\"}},[t._v(\"#\")]),t._v(\" 7. 作出预测\")]),t._v(\" \"),a(\"p\",[t._v(\"现在我们的模型已经训练好了，我们要做出一些预测。让我们通过查看模型对低到高功率的均匀范围范围内的预测值来评估模型。\")]),t._v(\" \"),a(\"p\",[t._v(\"将以下函数添加到您的 script.js 文件中\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"testModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" normalizationData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labelMax \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" normalizationData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Generate predictions for a uniform range of numbers between 0 and 1;\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// We un-normalize the data by doing the inverse of the min-max scaling\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// that we did earlier.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tidy\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" xs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"linspace\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" preds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"predict\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"reshape\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormXs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormPreds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Un-normalize the data\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"unNormXs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" unNormPreds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" predictedPoints \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" Array\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"from\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"val\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" i\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" val\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"i\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" originalPoints \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"render\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"scatterplot\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Model Predictions vs Original Data\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      values\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"originalPoints\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" predictedPoints\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      series\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"original\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"predicted\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      xLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      yLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"MPG\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"300\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"20\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"21\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"22\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"23\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"24\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"25\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"26\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"27\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"28\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"29\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"30\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"31\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"32\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"33\")]),a(\"br\")])]),a(\"p\",[t._v(\"在上面的函数中需要注意的几件事。\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" xs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"linspace\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" preds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"predict\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"reshape\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\")])]),a(\"p\",[t._v(\"我们生成了 100 个新的“示例”以供模型使用。 \"),a(\"code\",[t._v(\"Model.predict\")]),t._v(\"是我们如何将这些示例输入到模型中。请注意，它们的形状必须与我们训练时的形状相似\"),a(\"code\",[t._v(\"([[num_examples，num_features_per_example])\")]),t._v(\"。\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Un-normalize the data\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormXs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormPreds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\")])]),a(\"p\",[t._v(\"为了使数据回到我们的原始范围（而不是 0-1），我们使用在归一化时计算出的值，但只是反转了运算。\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"unNormXs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" unNormPreds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])]),a(\"p\",[a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#tf.Tensor.dataSync\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\".dataSync（）\"),a(\"OutboundLink\")],1),t._v(\"是一种我们可以用来获取张量中存储的值的 typedarray 的方法。这使我们可以使用常规 JavaScript 处理这些值。这是通常首选的\"),a(\"a\",{attrs:{href:\"https://js.tensorflow.org/api/latest/#tf.Tensor.data\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\".data（）\"),a(\"OutboundLink\")],1),t._v(\"方法的同步版本。\")]),t._v(\" \"),a(\"p\",[t._v(\"最后，我们使用 tfjs-vis 绘制原始数据和模型的预测。\")]),t._v(\" \"),a(\"p\",[t._v(\"将以下代码添加到您的\"),a(\"code\",[t._v(\"run\")]),t._v(\"函数当中\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Make some predictions using the model and compare them to the\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// original data\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"testModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" tensorData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\")])]),a(\"p\",[t._v(\"刷新页面，模型完成训练后，您应该会看到类似以下的内容。\\n\"),a(\"img\",{attrs:{src:\"https://codelabs.developers.google.com/codelabs/tfjs-training-regression/img/fe610ff34708d4a.png\",alt:\"\"}})]),t._v(\" \"),a(\"p\",[t._v(\"恭喜你！您刚刚训练了一个简单的机器学习模型。它当前执行所谓的线性回归，该回归试图使一条线适应输入数据中出现的趋势。\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_8-主要收获\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_8-主要收获\"}},[t._v(\"#\")]),t._v(\" 8. 主要收获\")]),t._v(\" \"),a(\"p\",[t._v(\"训练机器学习模型的步骤包括：\")]),t._v(\" \"),a(\"ol\",[a(\"li\",[a(\"p\",[t._v(\"制定任务：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"是回归问题还是分类问题？\")]),t._v(\" \"),a(\"li\",[t._v(\"可以通过有监督的学习还是无监督的学习来完成？\")]),t._v(\" \"),a(\"li\",[t._v(\"输入数据的形状是什么？输出数据应该是什么样？\")])])]),t._v(\" \"),a(\"li\",[a(\"p\",[t._v(\"准备数据：\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"清理数据并在可能的情况下手动检查其是否有图案\")]),t._v(\" \"),a(\"li\",[t._v(\"在将数据用于训练之前先对其进行清洗\")]),t._v(\" \"),a(\"li\",[t._v(\"将您的数据标准化到神经网络的合理范围内。通常 0-1 或-1-1 是数字数据的良好范围\")]),t._v(\" \"),a(\"li\",[t._v(\"将数据转换为张量\")])])]),t._v(\" \"),a(\"li\",[a(\"p\",[t._v(\"建立并运行模型\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"使用\"),a(\"code\",[t._v(\"tf.sequential\")]),t._v(\"或\"),a(\"code\",[t._v(\"tf.model\")]),t._v(\"定义模型，然后使用\"),a(\"code\",[t._v(\"tf.layers.*\")]),t._v(\"向其中添加层\")]),t._v(\" \"),a(\"li\",[t._v(\"选择一个优化器（\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#optimizer\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"adam\"),a(\"OutboundLink\")],1),t._v(\"是一个很好的优化器），以及批处理大小和时期数之类的参数。\")]),t._v(\" \"),a(\"li\",[t._v(\"选择一个适合您问题的\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#loss\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"损失函数\"),a(\"OutboundLink\")],1),t._v(\"，以及一个准确性指标以帮助您评估进度。 \"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#MSE\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"meanSquaredError\"),a(\"OutboundLink\")],1),t._v(\"是回归问题的常见损失函数。\")]),t._v(\" \"),a(\"li\",[t._v(\"监视训练以查看损失是否正在减少\")])])]),t._v(\" \"),a(\"li\",[a(\"p\",[t._v(\"评估模型\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"为模型选择一个评估指标，您可以在训练过程中对其进行监控。训练完毕后，请尝试进行一些测试预测，以提高预测质量。\")])])])]),t._v(\" \"),a(\"h3\",{attrs:{id:\"主要代码\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#主要代码\"}},[t._v(\"#\")]),t._v(\" 主要代码\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-html line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-html\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- index.html --\\x3e\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token doctype\"}},[t._v(\"<!DOCTYPE html>\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"html\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"head\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"title\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"TensorFlow.js Tutorial\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"title\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import TensorFlow.js --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import tfjs-vis --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis@1.0.2/dist/tfjs-vis.umd.min.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- Import the main script file --\\x3e\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"script\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"src\")]),a(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')]),t._v(\"script.js\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v('\"')])]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token script\"}}),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"script\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"head\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"body\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"body\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"html\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\")])]),a(\"p\",[a(\"code\",[t._v(\"script.js\")]),t._v(\"的代码:\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[t._v(\"console\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"log\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Hello TensorFlow\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" carsDataReq \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fetch\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"https://storage.googleapis.com/tfjs-tutorials/carsData.json\"')]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" carsData \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" carsDataReq\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"json\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" cleaned \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" carsData\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"car\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Miles_per_Gallon\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"filter\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"car\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"&&\")]),t._v(\" car\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" cleaned\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"run\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Load and plot the original input data that we are going to train on.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" data \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" values \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"render\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"scatterplot\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower v MPG\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      values\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      xLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      yLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"MPG\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"300\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Create the model\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" model \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"createModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"show\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"modelSummary\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Model Summary\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    model\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Convert the data to a form we can use for training.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" tensorData \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"convertToTensor\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tensorData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Train the model\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"trainModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// More code will be added below\")]),t._v(\"\\n  console\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"log\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Done Training\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Make some predictions using the model and compare them to the\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// original data\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"testModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" tensorData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"createModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Create a sequential model\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" model \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sequential\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Add a single input layer\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      inputShape\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      useBias\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Add an output layer\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      useBias\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n * Convert the input data to tensors that we can use for machine\\n * learning. We will also do the important best practices of _shuffling_\\n * the data and _normalizing_ the data\\n * MPG on the y-axis.\\n */\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"convertToTensor\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"data\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Wrapping these calculations in a tidy will dispose any\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// intermediate tensors.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tidy\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Step 1. Shuffle the data\")]),t._v(\"\\n    tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"shuffle\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Step 2. Convert data to Tensor\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelTensor \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tensor2d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//Step 3. Normalize the data to the range 0 - 1 using min-max scaling\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" inputMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMax \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"max\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" labelMin \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"min\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedInputs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputTensor\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" normalizedLabels \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" labelTensor\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"div\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedInputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labels\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" normalizedLabels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Return the min/max bounds so we can use them later.\")]),t._v(\"\\n      inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"async\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"trainModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Prepare the model for training.\")]),t._v(\"\\n  model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"compile\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    optimizer\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"adam\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    loss\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"losses\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"meanSquaredError\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    metrics\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" batchSize \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"32\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" epochs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"50\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"await\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fit\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labels\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    batchSize\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    epochs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    shuffle\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"show\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"fitCallbacks\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Training Performance\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"loss\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mse\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"200\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n        callbacks\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"onEpochEnd\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"function\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"testModel\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" normalizationData\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" labelMax \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" normalizationData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Generate predictions for a uniform range of numbers between 0 and 1;\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// We un-normalize the data by doing the inverse of the min-max scaling\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// that we did earlier.\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"tidy\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" xs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"linspace\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" preds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"predict\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"reshape\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"100\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormXs \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" unNormPreds \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mul\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMax\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"sub\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"labelMin\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// Un-normalize the data\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"unNormXs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" unNormPreds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dataSync\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" predictedPoints \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" Array\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"from\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"xs\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"val\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" i\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" val\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" preds\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"i\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"const\")]),t._v(\" originalPoints \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" inputData\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token parameter\"}},[t._v(\"d\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=>\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    x\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"horsepower\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    y\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" d\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mpg\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  tfvis\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"render\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"scatterplot\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      name\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Model Predictions vs Original Data\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      values\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"originalPoints\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" predictedPoints\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      series\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"original\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"predicted\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n      xLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"Horsepower\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      yLabel\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"MPG\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n      height\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"300\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n    \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n  \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\ndocument\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addEventListener\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"DOMContentLoaded\"')]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" run\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"20\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"21\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"22\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"23\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"24\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"25\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"26\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"27\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"28\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"29\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"30\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"31\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"32\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"33\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"34\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"35\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"36\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"37\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"38\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"39\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"40\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"41\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"42\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"43\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"44\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"45\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"46\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"47\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"48\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"49\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"50\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"51\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"52\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"53\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"54\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"55\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"56\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"57\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"58\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"59\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"60\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"61\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"62\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"63\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"64\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"65\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"66\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"67\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"68\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"69\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"70\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"71\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"72\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"73\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"74\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"75\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"76\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"77\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"78\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"79\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"80\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"81\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"82\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"83\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"84\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"85\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"86\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"87\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"88\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"89\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"90\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"91\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"92\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"93\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"94\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"95\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"96\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"97\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"98\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"99\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"100\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"101\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"102\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"103\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"104\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"105\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"106\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"107\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"108\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"109\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"110\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"111\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"112\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"113\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"114\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"115\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"116\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"117\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"118\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"119\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"120\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"121\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"122\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"123\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"124\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"125\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"126\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"127\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"128\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"129\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"130\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"131\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"132\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"133\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"134\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"135\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"136\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"137\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"138\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"139\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"140\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"141\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"142\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"143\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"144\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"145\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"146\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"147\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"148\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"149\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"150\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"151\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"152\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"153\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"154\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"155\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"156\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"157\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"158\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"159\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"160\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"161\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"162\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"163\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"164\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"165\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"166\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"167\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"168\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"169\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"170\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"171\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"172\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"173\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"174\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"175\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"176\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"177\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"178\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"179\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"180\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"181\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"182\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"183\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"184\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"185\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"186\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"187\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"188\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"189\")]),a(\"br\")])]),a(\"p\",[t._v(\"运行后的结果:\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"_9-额外：尝试的事情\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_9-额外：尝试的事情\"}},[t._v(\"#\")]),t._v(\" 9.额外：尝试的事情\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"尝试更改 epochs 变量。在图表变平之前，您需要多少个 epochs。\")]),t._v(\" \"),a(\"li\",[t._v(\"尝试增加隐藏层中的 units 数量。\")]),t._v(\" \"),a(\"li\",[t._v(\"尝试在添加的第一个隐藏层和最终输出层之间添加更多隐藏层。这些额外层的代码应如下所示\"),a(\"div\",{staticClass:\"language-js line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-js\"}},[a(\"code\",[t._v(\"model\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"add\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"dense\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\" units\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"50\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" activation\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"sigmoid\"')]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\")])])])]),t._v(\" \"),a(\"p\",[t._v(\"这些隐藏层最重要的新事物是它们引入了非线性激活函数，在这种情况下为\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/glossary/#sigmoid_function\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"S 型\"),a(\"OutboundLink\")],1),t._v(\"激活。要了解有关激活功能的更多信息，\"),a(\"a\",{attrs:{href:\"https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"请参阅本文\"),a(\"OutboundLink\")],1),t._v(\"。\")]),t._v(\" \"),a(\"p\",[t._v(\"看看是否可以使模型产生如下图所示的输出:\")]),t._v(\" \"),a(\"p\",[a(\"img\",{attrs:{src:\"https://codelabs.developers.google.com/codelabs/tfjs-training-regression/img/a21c5e6537cf81d.png\",alt:\"\"}})])])}),[],!1,null,null,null);s.default=e.exports}}]);","extractedComments":[]}